# llama-server-helper-ui
This project will help you view performance metrics and try out different args before deciding on a final model

Uses llama-server to run the model, grab it from the main repo. <br>

## Requirements
Cuda
Python
